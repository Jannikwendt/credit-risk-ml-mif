{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Walkthrough (Beginner)\n",
    "\n",
    "This project follows a structured machine learning workflow to predict credit risk. Here is a 10-step overview of the process:\n",
    "\n",
    "1.  **Environment Setup**: We load necessary libraries for data manipulation, visualization, and machine learning.\n",
    "2.  **Data Loading**: Historical loan data is imported from CSV files.\n",
    "3.  **Exploratory Data Analysis (EDA)**: We examine key variables like interest rates and debt-to-income ratios to understand their relationship with loan defaults.\n",
    "4.  **Feature Engineering**: New variables are created from existing ones to better capture borrower risk profiles (e.g., loan-to-income ratio).\n",
    "5.  **Data Splitting**: We split the data into training and validation sets to ensure we can test our models on unseen data.\n",
    "6.  **Preprocessing Pipeline**: A systematic process is established to handle missing values and convert categorical data into a format machines can understand.\n",
    "7.  **Model Training**: Multiple models (Logistic Regression, Decision Tree, Random Forest, and Gradient Boosting) are trained and tuned.\n",
    "8.  **Model Comparison**: We compare the performance of all models using standardized metrics like AUC and RMSE.\n",
    "9.  **Threshold Optimization**: We determine the best decision-making threshold to balance the cost of defaults against the benefit of approved loans.\n",
    "10. **Final Export**: All key results, models, and diagnostic charts are exported for reproducibility and reporting.\n",
    "\n",
    "# Glossary\n",
    "\n",
    "- **Target / Label**: The outcome we are trying to predict (in this case, whether a loan defaults).\n",
    "- **Feature**: An input variable used to make a prediction (e.g., borrower income, loan amount).\n",
    "- **Train / Val / Test**: Data splits used for building the model (Train), tuning it (Val), and final verification (Test).\n",
    "- **Pipeline**: A sequence of data processing steps and models chained together.\n",
    "- **One-Hot Encoding**: A method to convert categorical text data (like \"Home Ownership\") into numbers.\n",
    "- **Imputation**: The process of filling in missing data values with estimates (like the median).\n",
    "- **AUC (Area Under the Curve)**: A metric measuring how well the model distinguishes between \"default\" and \"non-default\" (higher is better).\n",
    "- **RMSE (Root Mean Square Error)**: A measure of the average difference between predicted probabilities and actual outcomes (lower is better).\n",
    "- **Threshold**: The \"cut-off\" probability used to decide if a loan should be approved or rejected.\n",
    "- **Confusion Matrix**: A table showing how many predictions were correct vs. incorrect for each class.\n",
    "- **Expected Value**: The average outcome of a decision when considering both the probability and the financial impact of different results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Map / Rubric Map\n",
    "\n",
    "- EDA (4 variables + business interpretation) → `# EDA (Core)` and `## EDA (Extension)`\n",
    "- Feature Engineering (≥4 features + economic rationale) → `## Feature Engineering (Economic Intuition)`\n",
    "- Train/Validation/Test usage (no leakage) → `## Train/Validation/Test Strategy (No Leakage)`\n",
    "- Models 0–3 + feature importance → `## Models (0–3)` and sub‑sections for each model\n",
    "- Model comparison → `## Model Comparison`\n",
    "- Threshold optimization (business) → `## Threshold Optimization (Business)`\n",
    "- Final test evaluation → `## Final Test Evaluation`\n",
    "- Conclusion → `## Conclusion`\n",
    "\n",
    "How to run: Restart Kernel & Run All.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric Compliance Map\n",
    "\n",
    "- EDA (4 variables + interpretation) → `## EDA (Core)` and `## EDA (Extension)`\n",
    "- Feature Engineering (≥4 features + rationale) → `## Feature Engineering (Economic Intuition)`\n",
    "- Train/Validation/Test protocol → `## Train/Validation/Test Strategy (No Leakage)`\n",
    "- Models & tuning → `## Models (0–3)` and model subsections\n",
    "- Model comparison → `## Model Comparison` + `outputs/model_comparison_validation.csv`\n",
    "- Threshold optimization → `## Threshold Optimization (Business)` + `outputs/threshold_sweep_validation.csv`\n",
    "- Business decisioning artifacts → `outputs/ev_curve_validation.png`, `outputs/approval_rate_curve_validation.png`\n",
    "- Confusion matrices → `outputs/confusion_matrix_validation_050.csv`, `outputs/confusion_matrix_validation_optimal.csv`\n",
    "- Model card → `outputs/model_card.md`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective & Dataset\n",
    "\n",
    "This notebook builds a supervised credit risk model to estimate default probability from historical loan data, following the course workflow and rubric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:14.930026Z",
     "iopub.status.busy": "2026-01-22T12:13:14.930026Z",
     "iopub.status.idle": "2026-01-22T12:13:14.936970Z",
     "shell.execute_reply": "2026-01-22T12:13:14.936259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Configure project-wide constants, paths, and payoff assumptions.\n",
    "# Inputs: Hardcoded strings and dictionary.\n",
    "# Outputs: Path objects, configuration variables, and PAYOFF dictionary.\n",
    "# Why it matters: Centralizing configuration ensures consistency across the notebook and defines the business logic for threshold optimization.\n",
    "\n",
    "# Config\n",
    "from pathlib import Path\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COL = \"default\"\n",
    "ID_COL = \"id\"\n",
    "TRAIN_PATH = \"../data/lending_club_train.csv\"\n",
    "TEST_PATH = \"../data/lending_club_test.csv\"\n",
    "\n",
    "REPO_ROOT = Path.cwd() if (Path.cwd() / \"data\").exists() else Path.cwd().parent\n",
    "OUTPUT_DIR = REPO_ROOT / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "assert (REPO_ROOT / \"data\" / \"lending_club_train.csv\").exists()\n",
    "\n",
    "PAYOFF = {\n",
    "    \"approve_nondefault\": 1,\n",
    "    \"approve_default\": -5,\n",
    "    \"reject_default\": 0,\n",
    "    \"reject_nondefault\": -1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:14.936970Z",
     "iopub.status.busy": "2026-01-22T12:13:14.936970Z",
     "iopub.status.idle": "2026-01-22T12:13:16.640745Z",
     "shell.execute_reply": "2026-01-22T12:13:16.640745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Import necessary libraries and set display options.\n",
    "# Inputs: None.\n",
    "# Outputs: Loaded Python modules and updated pandas display settings.\n",
    "# Why it matters: This cell provides all the computational tools required for data processing, visualization, and modeling.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"FigureCanvasAgg is non-interactive.*\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:16.640745Z",
     "iopub.status.busy": "2026-01-22T12:13:16.640745Z",
     "iopub.status.idle": "2026-01-22T12:13:16.672684Z",
     "shell.execute_reply": "2026-01-22T12:13:16.672684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10129, 21), (2533, 21))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Load the training and testing datasets from CSV files.\n",
    "# Inputs: TRAIN_PATH and TEST_PATH strings.\n",
    "# Outputs: df_train and df_test pandas DataFrames.\n",
    "# Why it matters: Loading the raw data is the first step in the machine learning pipeline, enabling all subsequent analysis and modeling.\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "df_train.shape, df_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality & Target Balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:16.698851Z",
     "iopub.status.busy": "2026-01-22T12:13:16.698851Z",
     "iopub.status.idle": "2026-01-22T12:13:16.707371Z",
     "shell.execute_reply": "2026-01-22T12:13:16.707371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default\n",
       "0    0.807187\n",
       "1    0.192813\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Verify target column existence and check the balance of default vs. non-default cases.\n",
    "# Inputs: df_train DataFrame and TARGET_COL.\n",
    "# Outputs: Normalized value counts of the target variable.\n",
    "# Why it matters: Understanding the class distribution is critical for model evaluation, as imbalanced data can lead to biased model performance.\n",
    "\n",
    "assert TARGET_COL in df_train.columns\n",
    "assert set(np.unique(df_train[TARGET_COL])).issubset({0, 1})\n",
    "\n",
    "df_train[TARGET_COL].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Types\n",
    "\n",
    "| Variable name | Variable type | Why it matters for credit risk |\n",
    "| --- | --- | --- |\n",
    "| int_rate | numerical (continuous) | Higher rates often reflect higher borrower risk and expected loss. |\n",
    "| dti | numerical (continuous) | Higher debt-to-income suggests tighter cash flow and default risk. |\n",
    "| loan_amnt | numerical (continuous) | Larger loans increase exposure and repayment burden. |\n",
    "| grade | categorical (ordinal) | Encodes lender risk grading tied to expected default levels. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Core)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:16.708414Z",
     "iopub.status.busy": "2026-01-22T12:13:16.708414Z",
     "iopub.status.idle": "2026-01-22T12:13:16.841139Z",
     "shell.execute_reply": "2026-01-22T12:13:16.841139Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_train[\"int_rate\"], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Interest Rates\")\n",
    "plt.xlabel(\"Interest Rate\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:16.843903Z",
     "iopub.status.busy": "2026-01-22T12:13:16.842157Z",
     "iopub.status.idle": "2026-01-22T12:13:16.877769Z",
     "shell.execute_reply": "2026-01-22T12:13:16.877201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Compare interest rates between repaid and defaulted loans.\n",
    "# Inputs: df_train DataFrame with \"int_rate\" and \"default\" columns.\n",
    "# Outputs: A boxplot comparing interest rate distributions by default status.\n",
    "# Why it matters: This visualization confirms if higher interest rates are associated with higher default risk, validating its use as a feature.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x=\"default\", y=\"int_rate\", data=df_train)\n",
    "plt.title(\"Interest Rate vs Default\")\n",
    "plt.xlabel(\"Default (0 = Repaid, 1 = Default)\")\n",
    "plt.ylabel(\"Interest Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interest Rate (`int_rate`)**\n",
    "\n",
    "The interest rate reflects the lender’s assessment of borrower risk at origination.\n",
    "The distribution shows a wide range of rates, with higher interest rates being more frequent among loans that eventually defaulted.\n",
    "This suggests that interest rates already embed risk information and are strongly associated with default probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:16.878545Z",
     "iopub.status.busy": "2026-01-22T12:13:16.878545Z",
     "iopub.status.idle": "2026-01-22T12:13:16.993916Z",
     "shell.execute_reply": "2026-01-22T12:13:16.993916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Visualize the distribution of Debt-to-Income (DTI) ratios.\n",
    "# Inputs: df_train[\"dti\"].\n",
    "# Outputs: A histogram plot of DTI ratios.\n",
    "# Why it matters: High DTI is a classic indicator of financial stress and increased default probability.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_train[\"dti\"], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Debt-to-Income Ratio (DTI)\")\n",
    "plt.xlabel(\"DTI\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:16.993916Z",
     "iopub.status.busy": "2026-01-22T12:13:16.993916Z",
     "iopub.status.idle": "2026-01-22T12:13:17.028895Z",
     "shell.execute_reply": "2026-01-22T12:13:17.028895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Compare DTI distributions between repaid and defaulted loans.\n",
    "# Inputs: df_train DataFrame with \"dti\" and \"default\" columns.\n",
    "# Outputs: A boxplot comparing DTI distributions by default status.\n",
    "# Why it matters: Identifying a higher DTI for defaulted loans supports the economic rationale for its inclusion in the model.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x=\"default\", y=\"dti\", data=df_train)\n",
    "plt.title(\"DTI vs Default\")\n",
    "plt.xlabel(\"Default (0 = Repaid, 1 = Default)\")\n",
    "plt.ylabel(\"Debt-to-Income Ratio\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debt-to-Income Ratio (`dti`)**\n",
    "\n",
    "The debt-to-income ratio measures the proportion of a borrower’s income already committed to debt obligations.\n",
    "Higher DTI values indicate greater financial strain and reduced repayment capacity.\n",
    "The analysis shows that defaulted loans tend to have higher DTI levels, confirming DTI as a key risk driver in credit decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.028895Z",
     "iopub.status.busy": "2026-01-22T12:13:17.028895Z",
     "iopub.status.idle": "2026-01-22T12:13:17.195421Z",
     "shell.execute_reply": "2026-01-22T12:13:17.195421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Visualize the distribution of loan amounts.\n",
    "# Inputs: df_train[\"loan_amnt\"].\n",
    "# Outputs: A histogram plot of loan amounts.\n",
    "# Why it matters: Understanding the scale of loans helps assess the overall risk exposure.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.histplot(df_train[\"loan_amnt\"], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Loan Amount\")\n",
    "plt.xlabel(\"Loan Amount\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.195421Z",
     "iopub.status.busy": "2026-01-22T12:13:17.195421Z",
     "iopub.status.idle": "2026-01-22T12:13:17.232697Z",
     "shell.execute_reply": "2026-01-22T12:13:17.232075Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Compare loan amount distributions by default status.\n",
    "# Inputs: df_train DataFrame with \"loan_amnt\" and \"default\" columns.\n",
    "# Outputs: A boxplot comparing loan amount distributions.\n",
    "# Why it matters: While loan size impacts loss severity, its relationship with default probability might be less direct than other factors.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x=\"default\", y=\"loan_amnt\", data=df_train)\n",
    "plt.title(\"Loan Amount vs Default\")\n",
    "plt.xlabel(\"Default (0 = Repaid, 1 = Default)\")\n",
    "plt.ylabel(\"Loan Amount\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loan Amount (`loan_amnt`)**\n",
    "\n",
    "The loan amount represents the size of the lender’s exposure rather than the borrower’s intrinsic credit quality.\n",
    "The distribution shows a concentration around mid-sized loans.\n",
    "The relationship with default appears weaker than for interest rate or DTI, suggesting that loan size alone is not a primary driver of default, but may interact with income or other risk factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.234733Z",
     "iopub.status.busy": "2026-01-22T12:13:17.234733Z",
     "iopub.status.idle": "2026-01-22T12:13:17.271124Z",
     "shell.execute_reply": "2026-01-22T12:13:17.270428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Show the frequency of different credit grades in the data.\n",
    "# Inputs: df_train[\"grade\"].\n",
    "# Outputs: A count plot of borrowers per credit grade.\n",
    "# Why it matters: Credit grade is a synthesized risk measure provided by the lender; understanding its distribution is key for assessment.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=\"grade\", data=df_train, order=sorted(df_train[\"grade\"].unique()))\n",
    "plt.title(\"Distribution of Credit Grades\")\n",
    "plt.xlabel(\"Credit Grade\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.273261Z",
     "iopub.status.busy": "2026-01-22T12:13:17.273261Z",
     "iopub.status.idle": "2026-01-22T12:13:17.306746Z",
     "shell.execute_reply": "2026-01-22T12:13:17.306746Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Calculate and plot the average default rate for each credit grade.\n",
    "# Inputs: df_train DataFrame grouped by \"grade\".\n",
    "# Outputs: A bar plot of default rates by grade.\n",
    "# Why it matters: This highlights the strong monotonic relationship between internal risk grades and actual default outcomes.\n",
    "\n",
    "grade_default_rate = (\n",
    "    df_train\n",
    "    .groupby(\"grade\")[\"default\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=\"grade\", y=\"default\", data=grade_default_rate,\n",
    "            order=sorted(grade_default_rate[\"grade\"]))\n",
    "plt.title(\"Average Default Rate by Credit Grade\")\n",
    "plt.xlabel(\"Credit Grade\")\n",
    "plt.ylabel(\"Default Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit Grade (`grade`)**\n",
    "\n",
    "The credit grade is an ordinal categorical variable summarizing the lender’s internal credit assessment at origination.\n",
    "Lower grades are associated with significantly higher default rates, while higher grades exhibit substantially lower default frequencies.\n",
    "This clear monotonic relationship confirms that `grade` is one of the strongest predictors of default and validates its economic relevance in credit risk modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.309126Z",
     "iopub.status.busy": "2026-01-22T12:13:17.309126Z",
     "iopub.status.idle": "2026-01-22T12:13:17.314224Z",
     "shell.execute_reply": "2026-01-22T12:13:17.314224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'loan_amnt',\n",
       " 'term',\n",
       " 'int_rate',\n",
       " 'installment',\n",
       " 'annual_inc',\n",
       " 'emp_length',\n",
       " 'dti',\n",
       " 'delinq_2yrs',\n",
       " 'inq_last_6mths',\n",
       " 'open_acc',\n",
       " 'pub_rec',\n",
       " 'revol_bal',\n",
       " 'revol_util',\n",
       " 'total_acc']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Identify and list all numerical columns for correlation analysis.\n",
    "# Inputs: df_train DataFrame.\n",
    "# Outputs: A list of numerical column names (num_cols).\n",
    "# Why it matters: Automated column selection ensures all relevant numeric features are included in the correlation analysis.\n",
    "\n",
    "# Select numerical columns only\n",
    "num_cols = df_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Exclude target itself\n",
    "num_cols = [c for c in num_cols if c != \"default\"]\n",
    "\n",
    "num_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.316049Z",
     "iopub.status.busy": "2026-01-22T12:13:17.316049Z",
     "iopub.status.idle": "2026-01-22T12:13:17.330660Z",
     "shell.execute_reply": "2026-01-22T12:13:17.330660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correlation with Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>int_rate</th>\n",
       "      <td>0.251016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <td>0.169246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dti</th>\n",
       "      <td>0.102043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <td>0.073969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revol_util</th>\n",
       "      <td>0.072172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>0.068005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>installment</th>\n",
       "      <td>0.054795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.042234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pub_rec</th>\n",
       "      <td>0.037639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <td>0.017844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>open_acc</th>\n",
       "      <td>0.014867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emp_length</th>\n",
       "      <td>0.005045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revol_bal</th>\n",
       "      <td>-0.019908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_acc</th>\n",
       "      <td>-0.027689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annual_inc</th>\n",
       "      <td>-0.046853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Correlation with Default\n",
       "int_rate                        0.251016\n",
       "term                            0.169246\n",
       "dti                             0.102043\n",
       "inq_last_6mths                  0.073969\n",
       "revol_util                      0.072172\n",
       "loan_amnt                       0.068005\n",
       "installment                     0.054795\n",
       "id                              0.042234\n",
       "pub_rec                         0.037639\n",
       "delinq_2yrs                     0.017844\n",
       "open_acc                        0.014867\n",
       "emp_length                      0.005045\n",
       "revol_bal                      -0.019908\n",
       "total_acc                      -0.027689\n",
       "annual_inc                     -0.046853"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Calculate the linear correlation between each numerical feature and the default target.\n",
    "# Inputs: df_train numerical columns.\n",
    "# Outputs: A sorted list of correlations with the default indicator.\n",
    "# Why it matters: This identifies which individual variables have the strongest linear relationship with loan default.\n",
    "\n",
    "corr_with_target = (\n",
    "    df_train[num_cols + [\"default\"]]\n",
    "    .corr()[\"default\"]\n",
    "    .drop(\"default\")\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "corr_with_target.to_frame(name=\"Correlation with Default\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (Economic Intuition)\n",
    "\n",
    "### Feature Engineering Strategy\n",
    "\n",
    "Feature engineering was guided by economic intuition rather than brute-force expansion. Engineered variables capture distinct borrower-risk dimensions: affordability/burden, leverage/utilization, credit experience/stability, and nonlinear effects, while maintaining interpretability and model stability.\n",
    "\n",
    "| Feature | Intuition |\n",
    "| --- | --- |\n",
    "| installment_to_income | installment / annual_inc |\n",
    "| loan_to_income | loan_amnt / annual_inc |\n",
    "| revol_balance_util | revol_bal * revol_util |\n",
    "| revol_balance_to_income | revol_bal / annual_inc |\n",
    "| open_to_total_acc | open_acc / total_acc |\n",
    "| recent_inquiry_flag | (inq_last_6mths > 0) |\n",
    "| log_annual_inc | log(annual_inc) |\n",
    "| sqrt_dti | sqrt(dti) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.330660Z",
     "iopub.status.busy": "2026-01-22T12:13:17.330660Z",
     "iopub.status.idle": "2026-01-22T12:13:17.338514Z",
     "shell.execute_reply": "2026-01-22T12:13:17.338514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Prepare data by separating the target variable and dropping identifiers.\n",
    "# Inputs: df_train and df_test DataFrames.\n",
    "# Outputs: X (features), y (target), and X_test_final (features for testing).\n",
    "# Why it matters: Proper separation of inputs and outcomes is essential for the Scikit-Learn training process.\n",
    "\n",
    "ID_COL = \"id\" if \"id\" in df_train.columns else None\n",
    "\n",
    "y = df_train[TARGET_COL].astype(int)\n",
    "X = df_train.drop(columns=[TARGET_COL] + ([ID_COL] if ID_COL else []))\n",
    "\n",
    "if ID_COL is not None:\n",
    "    assert ID_COL not in X.columns\n",
    "\n",
    "X_test_final = df_test.drop(columns=([ID_COL] if ID_COL and ID_COL in df_test.columns else []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.338514Z",
     "iopub.status.busy": "2026-01-22T12:13:17.338514Z",
     "iopub.status.idle": "2026-01-22T12:13:17.349804Z",
     "shell.execute_reply": "2026-01-22T12:13:17.349804Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    annual_inc_safe = df[\"annual_inc\"].clip(lower=1)\n",
    "    total_acc_safe = df[\"total_acc\"].replace(0, np.nan)\n",
    "\n",
    "    df[\"installment_to_income\"] = df[\"installment\"] / annual_inc_safe\n",
    "    df[\"loan_to_income\"] = df[\"loan_amnt\"] / annual_inc_safe\n",
    "    df[\"revol_balance_util\"] = df[\"revol_bal\"] * df[\"revol_util\"]\n",
    "    df[\"revol_balance_to_income\"] = df[\"revol_bal\"] / annual_inc_safe\n",
    "    df[\"open_to_total_acc\"] = df[\"open_acc\"] / total_acc_safe\n",
    "    df[\"recent_inquiry_flag\"] = (df[\"inq_last_6mths\"] > 0).astype(int)\n",
    "    df[\"log_annual_inc\"] = np.log(annual_inc_safe)\n",
    "    df[\"sqrt_dti\"] = np.sqrt(df[\"dti\"].clip(lower=0))\n",
    "\n",
    "    return df\n",
    "\n",
    "engineered_cols = [\n",
    "    \"installment_to_income\",\n",
    "    \"loan_to_income\",\n",
    "    \"revol_balance_util\",\n",
    "    \"revol_balance_to_income\",\n",
    "    \"open_to_total_acc\",\n",
    "    \"recent_inquiry_flag\",\n",
    "    \"log_annual_inc\",\n",
    "    \"sqrt_dti\",\n",
    "]\n",
    "\n",
    "X_fe = add_engineered_features(X)\n",
    "X_test_fe = add_engineered_features(X_test_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.349804Z",
     "iopub.status.busy": "2026-01-22T12:13:17.349804Z",
     "iopub.status.idle": "2026-01-22T12:13:17.366151Z",
     "shell.execute_reply": "2026-01-22T12:13:17.365843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered columns: ['installment_to_income', 'loan_to_income', 'revol_balance_util', 'revol_balance_to_income', 'open_to_total_acc', 'recent_inquiry_flag', 'log_annual_inc', 'sqrt_dti']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "installment_to_income      0.0\n",
       "loan_to_income             0.0\n",
       "revol_balance_util         0.0\n",
       "revol_balance_to_income    0.0\n",
       "open_to_total_acc          0.0\n",
       "recent_inquiry_flag        0.0\n",
       "log_annual_inc             0.0\n",
       "sqrt_dti                   0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Inspect the statistics and missing values of the newly engineered features.\n",
    "# Inputs: X_fe engineered columns.\n",
    "# Outputs: Summary statistics (mean, std, min, max) and missing value percentages.\n",
    "# Why it matters: Verification ensure the features were created correctly and don't introduce excessive missing data.\n",
    "\n",
    "print(\"Engineered columns:\", engineered_cols)\n",
    "\n",
    "X_fe[engineered_cols].describe().T[[\"mean\", \"std\", \"min\", \"max\"]]\n",
    "\n",
    "X_fe[engineered_cols].isna().mean().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.366151Z",
     "iopub.status.busy": "2026-01-22T12:13:17.366151Z",
     "iopub.status.idle": "2026-01-22T12:13:17.450291Z",
     "shell.execute_reply": "2026-01-22T12:13:17.450291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Visualize the correlation matrix of numerical variables using a heatmap.\n",
    "# Inputs: df_train numerical columns and default indicator.\n",
    "# Outputs: A heatmap showing correlation coefficients between all numeric variables.\n",
    "# Why it matters: Heatmaps reveal multicollinearity (highly correlated inputs) and help identify which features are most related to the target.\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(\n",
    "    df_train[num_cols + [\"default\"]].corr(),\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title(\"Correlation Matrix (Numerical Variables)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation with Default (Numerical Variables)**\n",
    "\n",
    "As an extension of the initial EDA, we examine linear correlations between numerical variables and the default indicator.\n",
    "Variables such as interest rate and debt-to-income ratio show stronger positive correlations with default, while income-related variables tend to be negatively correlated.\n",
    "These results are consistent with economic intuition and support the relevance of these variables for credit risk modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.450291Z",
     "iopub.status.busy": "2026-01-22T12:13:17.450291Z",
     "iopub.status.idle": "2026-01-22T12:13:17.458352Z",
     "shell.execute_reply": "2026-01-22T12:13:17.458352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Segment the DTI variable into quartiles to prepare for non-linear analysis.\n",
    "# Inputs: df_train[\"dti\"].\n",
    "# Outputs: A new temporary column \"dti_quartile\" in df_train.\n",
    "# Why it matters: Discretizing continuous variables can sometimes reveal non-linear patterns that a simple correlation might miss.\n",
    "\n",
    "df_train[\"dti_quartile\"] = pd.qcut(df_train[\"dti\"], q=4, labels=[\"Q1 (Low)\", \"Q2\", \"Q3\", \"Q4 (High)\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.459360Z",
     "iopub.status.busy": "2026-01-22T12:13:17.459360Z",
     "iopub.status.idle": "2026-01-22T12:13:17.466087Z",
     "shell.execute_reply": "2026-01-22T12:13:17.466087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dti_quartile</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1 (Low)</td>\n",
       "      <td>0.151599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>0.161672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>0.212732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4 (High)</td>\n",
       "      <td>0.245358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dti_quartile   default\n",
       "0     Q1 (Low)  0.151599\n",
       "1           Q2  0.161672\n",
       "2           Q3  0.212732\n",
       "3    Q4 (High)  0.245358"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dti_default_by_quantile = (\n",
    "    df_train\n",
    "    .groupby(\"dti_quartile\", observed=False)[\"default\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "dti_default_by_quantile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.466087Z",
     "iopub.status.busy": "2026-01-22T12:13:17.466087Z",
     "iopub.status.idle": "2026-01-22T12:13:17.493461Z",
     "shell.execute_reply": "2026-01-22T12:13:17.492195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Plot the default rate across DTI quartiles.\n",
    "# Inputs: dti_default_by_quantile summary table.\n",
    "# Outputs: A bar plot showing default rate by DTI segment.\n",
    "# Why it matters: Visualizing the trend across quantiles provides a clear, non-technical proof of the risk associated with high DTI.\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(\n",
    "    x=\"dti_quartile\",\n",
    "    y=\"default\",\n",
    "    data=dti_default_by_quantile\n",
    ")\n",
    "plt.title(\"Default Rate by DTI Quartile\")\n",
    "plt.xlabel(\"DTI Quartile\")\n",
    "plt.ylabel(\"Average Default Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default Rate by DTI Quantiles**\n",
    "\n",
    "To capture potential nonlinear effects, we analyze default rates across quartiles of the debt-to-income ratio.\n",
    "Default probability increases monotonically from the lowest to the highest DTI quartile, indicating that borrower risk rises disproportionately at higher levels of indebtedness.\n",
    "This supports the inclusion of nonlinear transformations or interaction-based features in subsequent modeling steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.493461Z",
     "iopub.status.busy": "2026-01-22T12:13:17.493461Z",
     "iopub.status.idle": "2026-01-22T12:13:17.499068Z",
     "shell.execute_reply": "2026-01-22T12:13:17.499068Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.drop(columns=[\"dti_quartile\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.499068Z",
     "iopub.status.busy": "2026-01-22T12:13:17.499068Z",
     "iopub.status.idle": "2026-01-22T12:13:17.514474Z",
     "shell.execute_reply": "2026-01-22T12:13:17.514474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>purpose</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79044496</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>12.99</td>\n",
       "      <td>363.97</td>\n",
       "      <td>C</td>\n",
       "      <td>C2</td>\n",
       "      <td>49000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>15.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26376.0</td>\n",
       "      <td>85.4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43246030</td>\n",
       "      <td>15150.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.99</td>\n",
       "      <td>495.92</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>38000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>39.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19459.0</td>\n",
       "      <td>63.2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>641694</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>14.83</td>\n",
       "      <td>303.38</td>\n",
       "      <td>D</td>\n",
       "      <td>D3</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>12.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8078.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>moving</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70981628</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>9.75</td>\n",
       "      <td>192.90</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>Verified</td>\n",
       "      <td>25.53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>39.4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57792301</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>13.33</td>\n",
       "      <td>343.84</td>\n",
       "      <td>C</td>\n",
       "      <td>C3</td>\n",
       "      <td>96000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>13.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15979.0</td>\n",
       "      <td>53.3</td>\n",
       "      <td>36.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  loan_amnt  term  int_rate  installment grade sub_grade  \\\n",
       "0  79044496    16000.0  60.0     12.99       363.97     C        C2   \n",
       "1  43246030    15150.0  36.0     10.99       495.92     B        B4   \n",
       "2    641694    12800.0  60.0     14.83       303.38     D        D3   \n",
       "3  70981628     6000.0  36.0      9.75       192.90     B        B3   \n",
       "4  57792301    15000.0  60.0     13.33       343.84     C        C3   \n",
       "\n",
       "   annual_inc  emp_length home_ownership verification_status    dti  \\\n",
       "0     49000.0         5.0           RENT     Source Verified  15.94   \n",
       "1     38000.0         8.0       MORTGAGE        Not Verified  39.96   \n",
       "2     75000.0         0.0           RENT     Source Verified  12.43   \n",
       "3     70000.0         0.0       MORTGAGE            Verified  25.53   \n",
       "4     96000.0         3.0           RENT     Source Verified  13.44   \n",
       "\n",
       "   delinq_2yrs  inq_last_6mths  open_acc  pub_rec  revol_bal  revol_util  \\\n",
       "0          0.0             0.0       9.0      0.0    26376.0        85.4   \n",
       "1          1.0             1.0      10.0      0.0    19459.0        63.2   \n",
       "2          0.0             0.0      12.0      0.0     8078.0        35.0   \n",
       "3          1.0             1.0       9.0      0.0     2048.0        39.4   \n",
       "4          0.0             0.0      11.0      1.0    15979.0        53.3   \n",
       "\n",
       "   total_acc             purpose  default  \n",
       "0       19.0  debt_consolidation        0  \n",
       "1       38.0         credit_card        0  \n",
       "2       13.0              moving        0  \n",
       "3       16.0  debt_consolidation        0  \n",
       "4       36.0  debt_consolidation        0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Quickly inspect the first few rows of the training data.\n",
    "# Inputs: df_train DataFrame.\n",
    "# Outputs: Display of the top 5 rows.\n",
    "# Why it matters: A quick visual check ensures data loading and basic transformations were successful.\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.515592Z",
     "iopub.status.busy": "2026-01-22T12:13:17.515592Z",
     "iopub.status.idle": "2026-01-22T12:13:17.525027Z",
     "shell.execute_reply": "2026-01-22T12:13:17.525027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10129 entries, 0 to 10128\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   10129 non-null  int64  \n",
      " 1   loan_amnt            10129 non-null  float64\n",
      " 2   term                 10129 non-null  float64\n",
      " 3   int_rate             10129 non-null  float64\n",
      " 4   installment          10129 non-null  float64\n",
      " 5   grade                10129 non-null  object \n",
      " 6   sub_grade            10129 non-null  object \n",
      " 7   annual_inc           10129 non-null  float64\n",
      " 8   emp_length           10129 non-null  float64\n",
      " 9   home_ownership       10129 non-null  object \n",
      " 10  verification_status  10129 non-null  object \n",
      " 11  dti                  10129 non-null  float64\n",
      " 12  delinq_2yrs          10129 non-null  float64\n",
      " 13  inq_last_6mths       10129 non-null  float64\n",
      " 14  open_acc             10129 non-null  float64\n",
      " 15  pub_rec              10129 non-null  float64\n",
      " 16  revol_bal            10129 non-null  float64\n",
      " 17  revol_util           10129 non-null  float64\n",
      " 18  total_acc            10129 non-null  float64\n",
      " 19  purpose              10129 non-null  object \n",
      " 20  default              10129 non-null  int64  \n",
      "dtypes: float64(14), int64(2), object(5)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.526148Z",
     "iopub.status.busy": "2026-01-22T12:13:17.526148Z",
     "iopub.status.idle": "2026-01-22T12:13:17.531803Z",
     "shell.execute_reply": "2026-01-22T12:13:17.531803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       int64\n",
       "loan_amnt              float64\n",
       "term                   float64\n",
       "int_rate               float64\n",
       "installment            float64\n",
       "grade                   object\n",
       "sub_grade               object\n",
       "annual_inc             float64\n",
       "emp_length             float64\n",
       "home_ownership          object\n",
       "verification_status     object\n",
       "dti                    float64\n",
       "delinq_2yrs            float64\n",
       "inq_last_6mths         float64\n",
       "open_acc               float64\n",
       "pub_rec                float64\n",
       "revol_bal              float64\n",
       "revol_util             float64\n",
       "total_acc              float64\n",
       "purpose                 object\n",
       "default                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: List the data types for all columns in the training set.\n",
    "# Inputs: df_train DataFrame.\n",
    "# Outputs: Series of column names and their associated data types.\n",
    "# Why it matters: Ensuring columns have correct types (e.g., numeric vs. categorical) is vital for proper preprocessing.\n",
    "\n",
    "df_train.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Strategy (No Leakage)\n",
    "\n",
    "Training uses only `X_train`/`y_train`, validation is used for tuning, and the test set is held out for final evaluation. `X_test_final` is never used in any model `.fit()` calls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models (0–3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning grids are intentionally small to ensure fast execution and reproducible results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0 — Logistic Regression (Baseline)\n",
    "\n",
    "This baseline provides a simple, interpretable benchmark for credit risk prediction. Logistic Regression serves as a linear reference point for comparison with more complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.531803Z",
     "iopub.status.busy": "2026-01-22T12:13:17.531803Z",
     "iopub.status.idle": "2026-01-22T12:13:17.541390Z",
     "shell.execute_reply": "2026-01-22T12:13:17.541390Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_fe,\n",
    "    y,\n",
    "    test_size=0.25,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.541390Z",
     "iopub.status.busy": "2026-01-22T12:13:17.541390Z",
     "iopub.status.idle": "2026-01-22T12:13:17.546393Z",
     "shell.execute_reply": "2026-01-22T12:13:17.546393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Verify that the test set remains completely separate from the training and validation data.\n",
    "# Inputs: X_test_final, X_train, and X_val datasets.\n",
    "# Outputs: None (assertion check).\n",
    "# Why it matters: This prevents \"data leakage,\" ensuring the test evaluation is a truly unbiased assessment of model performance.\n",
    "\n",
    "assert X_test_final is not X_train\n",
    "assert X_test_final is not X_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.546393Z",
     "iopub.status.busy": "2026-01-22T12:13:17.546393Z",
     "iopub.status.idle": "2026-01-22T12:13:17.552486Z",
     "shell.execute_reply": "2026-01-22T12:13:17.552486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Build a systematic pipeline to handle missing values and scale/encode different data types.\n",
    "# Inputs: X_train column definitions.\n",
    "# Outputs: A \"preprocess\" ColumnTransformer object.\n",
    "# Why it matters: Standardizing data ensures that the machine learning models treat all features fairly and can handle real-world data issues like missing values.\n",
    "\n",
    "categorical_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols = [c for c in X_train.columns if c not in categorical_cols]\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.552486Z",
     "iopub.status.busy": "2026-01-22T12:13:17.552486Z",
     "iopub.status.idle": "2026-01-22T12:13:17.691713Z",
     "shell.execute_reply": "2026-01-22T12:13:17.691713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline fitted. Preprocess: num + cat. Model: LogisticRegression.\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "log_reg_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", log_reg),\n",
    "    ]\n",
    ")\n",
    "\n",
    "log_reg_pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fitted. Preprocess: num + cat. Model: LogisticRegression.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.691713Z",
     "iopub.status.busy": "2026-01-22T12:13:17.691713Z",
     "iopub.status.idle": "2026-01-22T12:13:17.698167Z",
     "shell.execute_reply": "2026-01-22T12:13:17.698167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Define a helper function to calculate key metrics (AUC, RMSE) for both training and validation sets.\n",
    "# Inputs: Trained pipeline and datasets.\n",
    "# Outputs: A dictionary containing performance metrics and the \"results_df\" placeholder.\n",
    "# Why it matters: This function ensures that all models are evaluated consistently, making their results directly comparable.\n",
    "\n",
    "def evaluate_model(model_name, pipeline, X_train, y_train, X_val, y_val):\n",
    "    train_probs = pipeline.predict_proba(X_train)[:, 1]\n",
    "    val_probs = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_probs))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_probs))\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, train_probs)\n",
    "    val_auc = roc_auc_score(y_val, val_probs)\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Train_RMSE\": train_rmse,\n",
    "        \"Val_RMSE\": val_rmse,\n",
    "        \"Train_AUC\": train_auc,\n",
    "        \"Val_AUC\": val_auc,\n",
    "        \"AUC_Gap\": train_auc - val_auc,\n",
    "        \"RMSE_Gap\": val_rmse - train_rmse,\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.698167Z",
     "iopub.status.busy": "2026-01-22T12:13:17.698167Z",
     "iopub.status.idle": "2026-01-22T12:13:17.731842Z",
     "shell.execute_reply": "2026-01-22T12:13:17.731842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>Train_AUC</th>\n",
       "      <th>Val_AUC</th>\n",
       "      <th>AUC_Gap</th>\n",
       "      <th>RMSE_Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.374962</td>\n",
       "      <td>0.379806</td>\n",
       "      <td>0.712319</td>\n",
       "      <td>0.690397</td>\n",
       "      <td>0.021921</td>\n",
       "      <td>0.004845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train_RMSE  Val_RMSE  Train_AUC   Val_AUC   AUC_Gap  \\\n",
       "0  Logistic Regression    0.374962  0.379806   0.712319  0.690397  0.021921   \n",
       "\n",
       "   RMSE_Gap  \n",
       "0  0.004845  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Evaluate the Logistic Regression model and store its metrics.\n",
    "# Inputs: log_reg_pipeline and datasets.\n",
    "# Outputs: Updated results_df with Logistic Regression metrics.\n",
    "# Why it matters: This establishes the first entry in our model comparison table.\n",
    "\n",
    "log_reg_metrics = evaluate_model(\n",
    "    \"Logistic Regression\",\n",
    "    log_reg_pipeline,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    ")\n",
    "\n",
    "results_df = pd.concat([results_df, pd.DataFrame([log_reg_metrics])], ignore_index=True)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 — Decision Tree (Tuned)\n",
    "\n",
    "Decision Trees capture nonlinearities and interactions among borrower characteristics. Because they can overfit, we tune key hyperparameters using the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.733585Z",
     "iopub.status.busy": "2026-01-22T12:13:17.733585Z",
     "iopub.status.idle": "2026-01-22T12:13:17.740021Z",
     "shell.execute_reply": "2026-01-22T12:13:17.739797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Set up a dedicated preprocessing pipeline for tree-based models (no scaling needed).\n",
    "# Inputs: X_train column definitions.\n",
    "# Outputs: A \"preprocess_tree\" ColumnTransformer object.\n",
    "# Why it matters: Tree-based models are naturally scale-invariant, so we only need to handle missing values and encoding.\n",
    "\n",
    "categorical_cols_tree = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols_tree = [c for c in X_train.columns if c not in categorical_cols_tree]\n",
    "\n",
    "numeric_transformer_tree = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer_tree = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer_tree, num_cols_tree),\n",
    "        (\"cat\", categorical_transformer_tree, categorical_cols_tree),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:17.740777Z",
     "iopub.status.busy": "2026-01-22T12:13:17.740777Z",
     "iopub.status.idle": "2026-01-22T12:13:18.946207Z",
     "shell.execute_reply": "2026-01-22T12:13:18.946207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>val_auc</th>\n",
       "      <th>chosen_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>0.689931</td>\n",
       "      <td>0.667967</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.689931</td>\n",
       "      <td>0.667967</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.689931</td>\n",
       "      <td>0.667967</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.715711</td>\n",
       "      <td>0.659281</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>0.715711</td>\n",
       "      <td>0.659281</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>0.719240</td>\n",
       "      <td>0.656571</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>0.712199</td>\n",
       "      <td>0.656276</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0.720761</td>\n",
       "      <td>0.654881</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0.736669</td>\n",
       "      <td>0.648326</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.741372</td>\n",
       "      <td>0.646720</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_samples_leaf  train_auc   val_auc  chosen_flag\n",
       "0           3                50   0.689931  0.667967         True\n",
       "1           3               100   0.689931  0.667967        False\n",
       "2           3               200   0.689931  0.667967        False\n",
       "11         10               200   0.715711  0.659281        False\n",
       "8           7               200   0.715711  0.659281        False\n",
       "3           5                50   0.719240  0.656571        False\n",
       "5           5               200   0.712199  0.656276        False\n",
       "4           5               100   0.720761  0.654881        False\n",
       "7           7               100   0.736669  0.648326        False\n",
       "10         10               100   0.741372  0.646720        False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Optimize Decision Tree hyperparameters by testing different depth and leaf size combinations.\n",
    "# Inputs: Param grid for max_depth and min_samples_leaf.\n",
    "# Outputs: dt_tuning_df containing results for all combinations.\n",
    "# Why it matters: Tuning prevents the tree from becoming too complex (overfitting) or too simple (underfitting).\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"min_samples_leaf\": [50, 100, 200],\n",
    "}\n",
    "\n",
    "dt_tuning_results = []\n",
    "\n",
    "for max_depth in param_grid[\"max_depth\"]:\n",
    "    for min_samples_leaf in param_grid[\"min_samples_leaf\"]:\n",
    "        tree_model = DecisionTreeClassifier(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "\n",
    "        tree_pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"preprocess\", preprocess_tree),\n",
    "                (\"model\", tree_model),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        tree_pipeline.fit(X_train, y_train)\n",
    "        train_probs = tree_pipeline.predict_proba(X_train)[:, 1]\n",
    "        val_probs = tree_pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        dt_tuning_results.append(\n",
    "            {\n",
    "                \"max_depth\": max_depth,\n",
    "                \"min_samples_leaf\": min_samples_leaf,\n",
    "                \"train_auc\": roc_auc_score(y_train, train_probs),\n",
    "                \"val_auc\": roc_auc_score(y_val, val_probs),\n",
    "                \"chosen_flag\": False,\n",
    "            }\n",
    "        )\n",
    "\n",
    "dt_tuning_df = pd.DataFrame(dt_tuning_results)\n",
    "\n",
    "best_dt_idx = dt_tuning_df[\"val_auc\"].idxmax()\n",
    "dt_tuning_df.loc[best_dt_idx, \"chosen_flag\"] = True\n",
    "best_dt_params = dt_tuning_df.loc[best_dt_idx, [\"max_depth\", \"min_samples_leaf\"]].to_dict()\n",
    "\n",
    "# Keep compatibility with prior naming\n",
    "best_params = best_dt_params\n",
    "tuning_df = dt_tuning_df\n",
    "\n",
    "dt_tuning_df.sort_values(\"val_auc\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:18.947853Z",
     "iopub.status.busy": "2026-01-22T12:13:18.947853Z",
     "iopub.status.idle": "2026-01-22T12:13:19.012008Z",
     "shell.execute_reply": "2026-01-22T12:13:19.012008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline fitted. Preprocess: num + cat. Model: DecisionTreeClassifier.\n"
     ]
    }
   ],
   "source": [
    "# Goal: Train the Decision Tree model using the best identified parameters.\n",
    "# Inputs: Optimal max_depth and min_samples_leaf.\n",
    "# Outputs: A fitted best_tree_pipeline object.\n",
    "# Why it matters: This completes the training of our second model candidate using an optimized configuration.\n",
    "\n",
    "best_params = tuning_df.loc[tuning_df[\"val_auc\"].idxmax()].to_dict()\n",
    "\n",
    "best_tree = DecisionTreeClassifier(\n",
    "    max_depth=int(best_params[\"max_depth\"]),\n",
    "    min_samples_leaf=int(best_params[\"min_samples_leaf\"]),\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "best_tree_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess_tree),\n",
    "        (\"model\", best_tree),\n",
    "    ]\n",
    ")\n",
    "\n",
    "best_tree_pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fitted. Preprocess: num + cat. Model: DecisionTreeClassifier.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:19.014318Z",
     "iopub.status.busy": "2026-01-22T12:13:19.014318Z",
     "iopub.status.idle": "2026-01-22T12:13:19.043822Z",
     "shell.execute_reply": "2026-01-22T12:13:19.043494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>Train_AUC</th>\n",
       "      <th>Val_AUC</th>\n",
       "      <th>AUC_Gap</th>\n",
       "      <th>RMSE_Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.374962</td>\n",
       "      <td>0.379806</td>\n",
       "      <td>0.712319</td>\n",
       "      <td>0.690397</td>\n",
       "      <td>0.021921</td>\n",
       "      <td>0.004845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.379684</td>\n",
       "      <td>0.383817</td>\n",
       "      <td>0.689931</td>\n",
       "      <td>0.667967</td>\n",
       "      <td>0.021965</td>\n",
       "      <td>0.004133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train_RMSE  Val_RMSE  Train_AUC   Val_AUC   AUC_Gap  \\\n",
       "0  Logistic Regression    0.374962  0.379806   0.712319  0.690397  0.021921   \n",
       "1        Decision Tree    0.379684  0.383817   0.689931  0.667967  0.021965   \n",
       "\n",
       "   RMSE_Gap  \n",
       "0  0.004845  \n",
       "1  0.004133  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Evaluate the optimized Decision Tree and add its metrics to the comparison table.\n",
    "# Inputs: best_tree_pipeline and datasets.\n",
    "# Outputs: Updated results_df with Decision Tree metrics.\n",
    "# Why it matters: This allows us to see if the Decision Tree outperforms the Logistic Regression baseline.\n",
    "\n",
    "dt_metrics = evaluate_model(\n",
    "    \"Decision Tree\",\n",
    "    best_tree_pipeline,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    ")\n",
    "\n",
    "results_df = pd.concat([results_df, pd.DataFrame([dt_metrics])], ignore_index=True)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:19.043822Z",
     "iopub.status.busy": "2026-01-22T12:13:19.043822Z",
     "iopub.status.idle": "2026-01-22T12:13:19.080431Z",
     "shell.execute_reply": "2026-01-22T12:13:19.080431Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = best_tree_pipeline.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "importances = best_tree_pipeline.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    {\"feature\": feature_names, \"importance\": importances}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(\n",
    "    x=\"importance\",\n",
    "    y=\"feature\",\n",
    "    data=importance_df.head(10),\n",
    "    orient=\"h\",\n",
    ")\n",
    "plt.title(\"Top 10 Feature Importances (Decision Tree)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 — Random Forest (Bagging)\n",
    "\n",
    "Random Forest reduces variance via bagging and feature subsampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:13:19.082273Z",
     "iopub.status.busy": "2026-01-22T12:13:19.082273Z",
     "iopub.status.idle": "2026-01-22T12:14:02.145550Z",
     "shell.execute_reply": "2026-01-22T12:14:02.145550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline fitted. Preprocess: num + cat. Model: RandomForestClassifier.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>Train_AUC</th>\n",
       "      <th>Val_AUC</th>\n",
       "      <th>AUC_Gap</th>\n",
       "      <th>RMSE_Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.374962</td>\n",
       "      <td>0.379806</td>\n",
       "      <td>0.712319</td>\n",
       "      <td>0.690397</td>\n",
       "      <td>0.021921</td>\n",
       "      <td>0.004845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.379684</td>\n",
       "      <td>0.383817</td>\n",
       "      <td>0.689931</td>\n",
       "      <td>0.667967</td>\n",
       "      <td>0.021965</td>\n",
       "      <td>0.004133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.351933</td>\n",
       "      <td>0.379290</td>\n",
       "      <td>0.856029</td>\n",
       "      <td>0.693914</td>\n",
       "      <td>0.162116</td>\n",
       "      <td>0.027356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train_RMSE  Val_RMSE  Train_AUC   Val_AUC   AUC_Gap  \\\n",
       "0  Logistic Regression    0.374962  0.379806   0.712319  0.690397  0.021921   \n",
       "1        Decision Tree    0.379684  0.383817   0.689931  0.667967  0.021965   \n",
       "2        Random Forest    0.351933  0.379290   0.856029  0.693914  0.162116   \n",
       "\n",
       "   RMSE_Gap  \n",
       "0  0.004845  \n",
       "1  0.004133  \n",
       "2  0.027356  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Tune and train a Random Forest model, which uses an ensemble of trees to improve performance.\n",
    "# Inputs: X_train and y_train datasets, RF hyperparameter grid.\n",
    "# Outputs: A fitted rf_best_pipeline and updated results_df.\n",
    "# Why it matters: Random Forests are often more robust and accurate than single trees because they average out individual errors.\n",
    "\n",
    "categorical_cols_rf = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols_rf = [c for c in X_train.columns if c not in categorical_cols_rf]\n",
    "\n",
    "numeric_transformer_rf = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer_rf = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess_rf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer_rf, num_cols_rf),\n",
    "        (\"cat\", categorical_transformer_rf, categorical_cols_rf),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf_grid = {\n",
    "    \"n_estimators\": [200, 500],\n",
    "    \"max_depth\": [None, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 20, 50],\n",
    "    \"max_features\": [\"sqrt\", 0.5],\n",
    "}\n",
    "\n",
    "rf_tuning_results = []\n",
    "\n",
    "for n_estimators in rf_grid[\"n_estimators\"]:\n",
    "    for max_depth in rf_grid[\"max_depth\"]:\n",
    "        for min_samples_leaf in rf_grid[\"min_samples_leaf\"]:\n",
    "            for max_features in rf_grid[\"max_features\"]:\n",
    "                rf_model = RandomForestClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    max_features=max_features,\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    n_jobs=-1,\n",
    "                )\n",
    "\n",
    "                rf_pipeline = Pipeline(\n",
    "                    steps=[\n",
    "                        (\"preprocess\", preprocess_rf),\n",
    "                        (\"model\", rf_model),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                rf_pipeline.fit(X_train, y_train)\n",
    "                val_probs = rf_pipeline.predict_proba(X_val)[:, 1]\n",
    "                val_auc = roc_auc_score(y_val, val_probs)\n",
    "\n",
    "                rf_tuning_results.append(\n",
    "                    {\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"min_samples_leaf\": min_samples_leaf,\n",
    "                        \"max_features\": max_features,\n",
    "                        \"val_auc\": val_auc,\n",
    "                        \"chosen_flag\": False,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "rf_tuning_df = pd.DataFrame(rf_tuning_results)\n",
    "rf_tuning_df[\"depth_rank\"] = rf_tuning_df[\"max_depth\"].apply(lambda d: 999 if d is None else d)\n",
    "\n",
    "rf_sorted = rf_tuning_df.sort_values(\n",
    "    [\"val_auc\", \"n_estimators\", \"depth_rank\"],\n",
    "    ascending=[False, True, True],\n",
    ")\n",
    "\n",
    "best_rf_idx = rf_sorted.index[0]\n",
    "rf_tuning_df.loc[best_rf_idx, \"chosen_flag\"] = True\n",
    "best_rf_params = rf_tuning_df.loc[\n",
    "    best_rf_idx, [\"n_estimators\", \"max_depth\", \"min_samples_leaf\", \"max_features\"]\n",
    "].to_dict()\n",
    "\n",
    "if pd.isna(best_rf_params[\"max_depth\"]):\n",
    "    best_rf_params[\"max_depth\"] = None\n",
    "\n",
    "rf_tuning_df = rf_tuning_df.drop(columns=[\"depth_rank\"])\n",
    "\n",
    "rf_tuning_df.sort_values(\"val_auc\", ascending=False).head(10)\n",
    "\n",
    "rf_best_model = RandomForestClassifier(\n",
    "    n_estimators=int(best_rf_params[\"n_estimators\"]),\n",
    "    max_depth=best_rf_params[\"max_depth\"],\n",
    "    min_samples_leaf=int(best_rf_params[\"min_samples_leaf\"]),\n",
    "    max_features=best_rf_params[\"max_features\"],\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf_best_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess_rf),\n",
    "        (\"model\", rf_best_model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf_best_pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fitted. Preprocess: num + cat. Model: RandomForestClassifier.\")\n",
    "\n",
    "rf_metrics = evaluate_model(\n",
    "    \"Random Forest\",\n",
    "    rf_best_pipeline,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    ")\n",
    "\n",
    "results_df = pd.concat([results_df, pd.DataFrame([rf_metrics])], ignore_index=True)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:14:02.145550Z",
     "iopub.status.busy": "2026-01-22T12:14:02.145550Z",
     "iopub.status.idle": "2026-01-22T12:14:02.232355Z",
     "shell.execute_reply": "2026-01-22T12:14:02.232355Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_feature_names = rf_best_pipeline.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "rf_importances = rf_best_pipeline.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "rf_importance_df = pd.DataFrame(\n",
    "    {\"feature\": rf_feature_names, \"importance\": rf_importances}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(\n",
    "    x=\"importance\",\n",
    "    y=\"feature\",\n",
    "    data=rf_importance_df.head(10),\n",
    "    orient=\"h\",\n",
    ")\n",
    "plt.title(\"Top 10 Feature Importances (Random Forest)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Feature Importance\n",
    "\n",
    "The chart above shows which variables the model relies on most to predict loan defaults. Here is a plain-English interpretation of the top features:\n",
    "\n",
    "1.  **Interest Rate (`int_rate`)**: Borrowers with higher interest rates are more likely to default. This is often because the lender already identified them as higher risk at the time of the loan application.\n",
    "2.  **Credit Grade (`grade_...`)**: Internal risk grades assigned by the lender are strong predictors. Lower grades (e.g., E, F, G) typically show a higher risk of default.\n",
    "3.  **Debt-to-Income Ratio (`dti` or `sqrt_dti`)**: Borrowers who spend a larger portion of their income on debt payments are more likely to struggle with new loan repayments.\n",
    "4.  **Revolving Utilization (`revol_util`)**: Using a high percentage of available credit card limits suggests a borrower may be overextended.\n",
    "5.  **Inquiries in Last 6 Months (`inq_last_6mths`)**: A high number of recent credit applications can be a sign of financial distress or an urgent need for cash.\n",
    "6.  **Annual Income (`annual_inc` or `log_annual_inc`)**: Higher income generally correlates with a lower risk of default, as borrowers have more cushion to handle unexpected expenses.\n",
    "\n",
    "**Note**: These are *correlational* signals, not *causal*. For example, a high interest rate doesn't necessarily *cause* a default; rather, high-risk borrowers are both more likely to default and more likely to be charged higher rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 — Gradient Boosting (Boosting)\n",
    "\n",
    "Gradient Boosting reduces bias via sequential weak learners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:14:02.232355Z",
     "iopub.status.busy": "2026-01-22T12:14:02.232355Z",
     "iopub.status.idle": "2026-01-22T12:15:41.005966Z",
     "shell.execute_reply": "2026-01-22T12:15:41.005966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline fitted. Preprocess: num + cat. Model: GradientBoostingClassifier.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>Train_AUC</th>\n",
       "      <th>Val_AUC</th>\n",
       "      <th>AUC_Gap</th>\n",
       "      <th>RMSE_Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.374962</td>\n",
       "      <td>0.379806</td>\n",
       "      <td>0.712319</td>\n",
       "      <td>0.690397</td>\n",
       "      <td>0.021921</td>\n",
       "      <td>0.004845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.379684</td>\n",
       "      <td>0.383817</td>\n",
       "      <td>0.689931</td>\n",
       "      <td>0.667967</td>\n",
       "      <td>0.021965</td>\n",
       "      <td>0.004133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.351933</td>\n",
       "      <td>0.379290</td>\n",
       "      <td>0.856029</td>\n",
       "      <td>0.693914</td>\n",
       "      <td>0.162116</td>\n",
       "      <td>0.027356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.357419</td>\n",
       "      <td>0.380680</td>\n",
       "      <td>0.786562</td>\n",
       "      <td>0.693174</td>\n",
       "      <td>0.093389</td>\n",
       "      <td>0.023260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train_RMSE  Val_RMSE  Train_AUC   Val_AUC   AUC_Gap  \\\n",
       "0  Logistic Regression    0.374962  0.379806   0.712319  0.690397  0.021921   \n",
       "1        Decision Tree    0.379684  0.383817   0.689931  0.667967  0.021965   \n",
       "2        Random Forest    0.351933  0.379290   0.856029  0.693914  0.162116   \n",
       "3    Gradient Boosting    0.357419  0.380680   0.786562  0.693174  0.093389   \n",
       "\n",
       "   RMSE_Gap  \n",
       "0  0.004845  \n",
       "1  0.004133  \n",
       "2  0.027356  \n",
       "3  0.023260  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Tune and train a Gradient Boosting model to capture complex, non-linear patterns.\n",
    "# Inputs: X_train and y_train datasets, GB hyperparameter grid.\n",
    "# Outputs: A fitted gb_best_pipeline and updated results_df.\n",
    "# Why it matters: Gradient Boosting is a powerful ensemble method that often provides the highest accuracy in credit risk tasks.\n",
    "\n",
    "categorical_cols_gb = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "num_cols_gb = [c for c in X_train.columns if c not in categorical_cols_gb]\n",
    "\n",
    "numeric_transformer_gb = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer_gb = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess_gb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer_gb, num_cols_gb),\n",
    "        (\"cat\", categorical_transformer_gb, categorical_cols_gb),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gb_grid = {\n",
    "    \"n_estimators\": [200, 500],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"max_depth\": [2, 3],\n",
    "    \"subsample\": [1.0],\n",
    "}\n",
    "\n",
    "gb_tuning_results = []\n",
    "\n",
    "for n_estimators in gb_grid[\"n_estimators\"]:\n",
    "    for learning_rate in gb_grid[\"learning_rate\"]:\n",
    "        for max_depth in gb_grid[\"max_depth\"]:\n",
    "            for subsample in gb_grid[\"subsample\"]:\n",
    "                gb_model = GradientBoostingClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth,\n",
    "                    subsample=subsample,\n",
    "                    random_state=RANDOM_STATE,\n",
    "                )\n",
    "\n",
    "                gb_pipeline = Pipeline(\n",
    "                    steps=[\n",
    "                        (\"preprocess\", preprocess_gb),\n",
    "                        (\"model\", gb_model),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                gb_pipeline.fit(X_train, y_train)\n",
    "                val_probs = gb_pipeline.predict_proba(X_val)[:, 1]\n",
    "                val_auc = roc_auc_score(y_val, val_probs)\n",
    "\n",
    "                gb_tuning_results.append(\n",
    "                    {\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"subsample\": subsample,\n",
    "                        \"val_auc\": val_auc,\n",
    "                        \"chosen_flag\": False,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "gb_tuning_df = pd.DataFrame(gb_tuning_results)\n",
    "\n",
    "gb_sorted = gb_tuning_df.sort_values(\n",
    "    [\"val_auc\", \"n_estimators\", \"max_depth\", \"learning_rate\"],\n",
    "    ascending=[False, True, True, True],\n",
    ")\n",
    "\n",
    "best_gb_idx = gb_sorted.index[0]\n",
    "gb_tuning_df.loc[best_gb_idx, \"chosen_flag\"] = True\n",
    "best_gb_params = gb_tuning_df.loc[\n",
    "    best_gb_idx, [\"n_estimators\", \"learning_rate\", \"max_depth\", \"subsample\"]\n",
    "].to_dict()\n",
    "\n",
    "gb_tuning_df.sort_values(\"val_auc\", ascending=False).head(10)\n",
    "\n",
    "gb_best_model = GradientBoostingClassifier(\n",
    "    n_estimators=int(best_gb_params[\"n_estimators\"]),\n",
    "    learning_rate=best_gb_params[\"learning_rate\"],\n",
    "    max_depth=int(best_gb_params[\"max_depth\"]),\n",
    "    subsample=best_gb_params[\"subsample\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "gb_best_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess_gb),\n",
    "        (\"model\", gb_best_model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gb_best_pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline fitted. Preprocess: num + cat. Model: GradientBoostingClassifier.\")\n",
    "\n",
    "gb_metrics = evaluate_model(\n",
    "    \"Gradient Boosting\",\n",
    "    gb_best_pipeline,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    ")\n",
    "\n",
    "results_df = pd.concat([results_df, pd.DataFrame([gb_metrics])], ignore_index=True)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:15:41.005966Z",
     "iopub.status.busy": "2026-01-22T12:15:41.005966Z",
     "iopub.status.idle": "2026-01-22T12:15:41.047243Z",
     "shell.execute_reply": "2026-01-22T12:15:41.047243Z"
    }
   },
   "outputs": [],
   "source": [
    "gb_feature_names = gb_best_pipeline.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "gb_importances = gb_best_pipeline.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "gb_importance_df = pd.DataFrame(\n",
    "    {\"feature\": gb_feature_names, \"importance\": gb_importances}\n",
    ").sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Goal: Visualize feature importances for the Gradient Boosting model.\n",
    "# Inputs: gb_best_pipeline model importances.\n",
    "# Outputs: A horizontal bar plot of top-10 features.\n",
    "# Why it matters: Comparing feature importance across models helps build a more reliable consensus on which variables truly drive risk.\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(\n",
    "    x=\"importance\",\n",
    "    y=\"feature\",\n",
    "    data=gb_importance_df.head(10),\n",
    "    orient=\"h\",\n",
    ")\n",
    "plt.title(\"Top 10 Feature Importances (Gradient Boosting)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:15:41.050211Z",
     "iopub.status.busy": "2026-01-22T12:15:41.050211Z",
     "iopub.status.idle": "2026-01-22T12:15:41.059809Z",
     "shell.execute_reply": "2026-01-22T12:15:41.059809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train_AUC</th>\n",
       "      <th>Val_AUC</th>\n",
       "      <th>AUC_Gap</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>RMSE_Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.856029</td>\n",
       "      <td>0.693914</td>\n",
       "      <td>0.162116</td>\n",
       "      <td>0.351933</td>\n",
       "      <td>0.379290</td>\n",
       "      <td>0.027356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.786562</td>\n",
       "      <td>0.693174</td>\n",
       "      <td>0.093389</td>\n",
       "      <td>0.357419</td>\n",
       "      <td>0.380680</td>\n",
       "      <td>0.023260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.712319</td>\n",
       "      <td>0.690397</td>\n",
       "      <td>0.021921</td>\n",
       "      <td>0.374962</td>\n",
       "      <td>0.379806</td>\n",
       "      <td>0.004845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.689931</td>\n",
       "      <td>0.667967</td>\n",
       "      <td>0.021965</td>\n",
       "      <td>0.379684</td>\n",
       "      <td>0.383817</td>\n",
       "      <td>0.004133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train_AUC   Val_AUC   AUC_Gap  Train_RMSE  Val_RMSE  \\\n",
       "0        Random Forest   0.856029  0.693914  0.162116    0.351933  0.379290   \n",
       "1    Gradient Boosting   0.786562  0.693174  0.093389    0.357419  0.380680   \n",
       "2  Logistic Regression   0.712319  0.690397  0.021921    0.374962  0.379806   \n",
       "3        Decision Tree   0.689931  0.667967  0.021965    0.379684  0.383817   \n",
       "\n",
       "   RMSE_Gap  \n",
       "0  0.027356  \n",
       "1  0.023260  \n",
       "2  0.004845  \n",
       "3  0.004133  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Aggregate and format metrics from all models for final comparison.\n",
    "# Inputs: results_df from all model evaluation steps.\n",
    "# Outputs: A sorted comparison_df showing AUC and RMSE for all candidates.\n",
    "# Why it matters: This table provides a clear, quantitative basis for choosing the final model to be used in production.\n",
    "\n",
    "comparison_df = results_df[\n",
    "    [\"Model\", \"Train_AUC\", \"Val_AUC\", \"AUC_Gap\", \"Train_RMSE\", \"Val_RMSE\", \"RMSE_Gap\"]\n",
    "].copy()\n",
    "\n",
    "comparison_df[[\"Train_AUC\", \"Val_AUC\", \"AUC_Gap\", \"Train_RMSE\", \"Val_RMSE\", \"RMSE_Gap\"]] = (\n",
    "    comparison_df[[\"Train_AUC\", \"Val_AUC\", \"AUC_Gap\", \"Train_RMSE\", \"Val_RMSE\", \"RMSE_Gap\"]].round(6)\n",
    ")\n",
    "\n",
    "comparison_df = comparison_df.sort_values(\"Val_AUC\", ascending=False).reset_index(drop=True)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the highest validation AUC is selected as the leading candidate. The AUC_Gap column highlights overfitting risk: larger gaps indicate stronger train–validation divergence. RMSE values provide a probability calibration check; small RMSE_Gap values suggest stable generalization. Overall improvements are modest but consistent with credit‑risk modeling norms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the final model for business thresholding based on validation AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:15:41.059809Z",
     "iopub.status.busy": "2026-01-22T12:15:41.059809Z",
     "iopub.status.idle": "2026-01-22T12:15:41.066858Z",
     "shell.execute_reply": "2026-01-22T12:15:41.066858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Random Forest'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Select the best-performing model (highest Validation AUC) for business optimization.\n",
    "# Inputs: comparison_df and all trained pipelines.\n",
    "# Outputs: FINAL_MODEL_NAME and FINAL_PIPELINE.\n",
    "# Why it matters: Automating the selection ensures that the most accurate model is always used for the final decisioning logic.\n",
    "\n",
    "FINAL_MODEL_NAME = comparison_df.loc[0, \"Model\"]\n",
    "\n",
    "FINAL_PIPELINE = {\n",
    "    \"Logistic Regression\": log_reg_pipeline,\n",
    "    \"Decision Tree\": best_tree_pipeline,\n",
    "    \"Random Forest\": rf_best_pipeline,\n",
    "    \"Gradient Boosting\": gb_best_pipeline,\n",
    "}[FINAL_MODEL_NAME]\n",
    "\n",
    "FINAL_MODEL_NAME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Optimization (Business)\n",
    "\n",
    "Positive class is `default = 1`, and the model outputs P(default). We make a lending decision by comparing the predicted default probability to a threshold: **approve** if P(default) < threshold, **reject** otherwise. The optimal threshold depends on business costs and benefits. Thresholds are chosen on the validation set; the test set is reserved for final evaluation only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Assumptions\n",
    "\n",
    "Because no monetary cost-benefit values are provided, we use a clear, outcome-based payoff matrix (defined in `PAYOFF`):\n",
    "\n",
    "- Approve & non‑default: +1\n",
    "- Approve & default: −5\n",
    "- Reject & default: 0\n",
    "- Reject & non‑default: −1\n",
    "\n",
    "Defaults are much more costly than missed opportunities, so the policy is intentionally conservative. These values are illustrative but economically reasonable for decision‑making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:15:41.068989Z",
     "iopub.status.busy": "2026-01-22T12:15:41.068989Z",
     "iopub.status.idle": "2026-01-22T12:15:41.368051Z",
     "shell.execute_reply": "2026-01-22T12:15:41.368051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline approvals: 2529, defaults among approved: 486, approval rate: 0.998\n",
      "Optimal approvals: 2076, defaults among approved: 303, approval rate: 0.820\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>multiplier</th>\n",
       "      <th>best_threshold</th>\n",
       "      <th>best_expected_value</th>\n",
       "      <th>best_approval_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-65.6</td>\n",
       "      <td>0.819582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>0.819582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.29</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.819582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   multiplier  best_threshold  best_expected_value  best_approval_rate\n",
       "0         0.8            0.29                -65.6            0.819582\n",
       "1         1.0            0.29                -14.0            0.819582\n",
       "2         1.2            0.29                 37.6            0.819582"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal: Determine the optimal classification threshold that maximizes business value based on payoff assumptions.\n",
    "# Inputs: FINAL_PIPELINE, validation set, and PAYOFF matrix.\n",
    "# Outputs: optimal_table, best_threshold, and threshold sensitivity analysis.\n",
    "# Why it matters: This translates a probability (ML) into a decision (Business), ensuring the model creates the most economic value.\n",
    "\n",
    "val_proba = FINAL_PIPELINE.predict_proba(X_val)[:, 1]\n",
    "\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "results_threshold = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    approve = val_proba < threshold\n",
    "\n",
    "    approve_nondefault = ((approve) & (y_val == 0)).sum()\n",
    "    approve_default = ((approve) & (y_val == 1)).sum()\n",
    "    reject_nondefault = ((~approve) & (y_val == 0)).sum()\n",
    "    reject_default = ((~approve) & (y_val == 1)).sum()\n",
    "\n",
    "    expected_value = (\n",
    "        approve_nondefault * PAYOFF[\"approve_nondefault\"]\n",
    "        + approve_default * PAYOFF[\"approve_default\"]\n",
    "        + reject_default * PAYOFF[\"reject_default\"]\n",
    "        + reject_nondefault * PAYOFF[\"reject_nondefault\"]\n",
    "    )\n",
    "\n",
    "    approval_rate = approve.mean()\n",
    "\n",
    "    results_threshold.append(\n",
    "        {\n",
    "            \"threshold\": threshold,\n",
    "            \"expected_value\": expected_value,\n",
    "            \"approval_rate\": approval_rate,\n",
    "            \"approve_nondefault\": approve_nondefault,\n",
    "            \"approve_default\": approve_default,\n",
    "            \"reject_nondefault\": reject_nondefault,\n",
    "            \"reject_default\": reject_default,\n",
    "        }\n",
    "    )\n",
    "\n",
    "threshold_df = pd.DataFrame(results_threshold)\n",
    "\n",
    "best_idx = threshold_df[\"expected_value\"].idxmax()\n",
    "best_threshold = threshold_df.loc[best_idx, \"threshold\"]\n",
    "best_expected_value = threshold_df.loc[best_idx, \"expected_value\"]\n",
    "best_approval_rate = threshold_df.loc[best_idx, \"approval_rate\"]\n",
    "\n",
    "# Expected value curve\n",
    "ev_fig, ev_ax = plt.subplots(figsize=(7, 4))\n",
    "ev_ax.plot(threshold_df[\"threshold\"], threshold_df[\"expected_value\"], marker=\"o\", markersize=3)\n",
    "ev_ax.axvline(best_threshold, color=\"red\", linestyle=\"--\", label=f\"Best threshold = {best_threshold:.2f}\")\n",
    "ev_ax.set_title(\"Expected Value vs Threshold (Validation)\")\n",
    "ev_ax.set_xlabel(\"Threshold\")\n",
    "ev_ax.set_ylabel(\"Expected Value\")\n",
    "ev_ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Approval rate curve\n",
    "approval_fig, approval_ax = plt.subplots(figsize=(7, 4))\n",
    "approval_ax.plot(threshold_df[\"threshold\"], threshold_df[\"approval_rate\"], marker=\"o\", markersize=3)\n",
    "approval_ax.axvline(best_threshold, color=\"red\", linestyle=\"--\", label=f\"Best threshold = {best_threshold:.2f}\")\n",
    "approval_ax.set_title(\"Approval Rate vs Threshold (Validation)\")\n",
    "approval_ax.set_xlabel(\"Threshold\")\n",
    "approval_ax.set_ylabel(\"Approval Rate\")\n",
    "approval_ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion-style tables at 0.50 and optimal threshold\n",
    "\n",
    "def decision_table(threshold):\n",
    "    approve = val_proba < threshold\n",
    "    approve_nondefault = int(((approve) & (y_val == 0)).sum())\n",
    "    approve_default = int(((approve) & (y_val == 1)).sum())\n",
    "    reject_nondefault = int(((~approve) & (y_val == 0)).sum())\n",
    "    reject_default = int(((~approve) & (y_val == 1)).sum())\n",
    "\n",
    "    table = pd.DataFrame(\n",
    "        {\n",
    "            \"Non-default\": [approve_nondefault, reject_nondefault],\n",
    "            \"Default\": [approve_default, reject_default],\n",
    "        },\n",
    "        index=[\"Approved\", \"Rejected\"],\n",
    "    )\n",
    "\n",
    "    approvals = approve.sum()\n",
    "    bad_approvals = approve_default\n",
    "    approval_rate = approvals / len(y_val)\n",
    "\n",
    "    return table, approvals, bad_approvals, approval_rate\n",
    "\n",
    "baseline_table, baseline_approvals, baseline_bad, baseline_rate = decision_table(0.50)\n",
    "optimal_table, optimal_approvals, optimal_bad, optimal_rate = decision_table(best_threshold)\n",
    "\n",
    "baseline_table, optimal_table\n",
    "print(f\"Baseline approvals: {baseline_approvals}, defaults among approved: {baseline_bad}, approval rate: {baseline_rate:.3f}\")\n",
    "print(f\"Optimal approvals: {optimal_approvals}, defaults among approved: {optimal_bad}, approval rate: {optimal_rate:.3f}\")\n",
    "\n",
    "# Sensitivity analysis: scale approve outcomes by ±20%\n",
    "\n",
    "sensitivity_rows = []\n",
    "for multiplier in [0.8, 1.0, 1.2]:\n",
    "    payoff = PAYOFF.copy()\n",
    "    payoff[\"approve_nondefault\"] = PAYOFF[\"approve_nondefault\"] * multiplier\n",
    "    payoff[\"approve_default\"] = PAYOFF[\"approve_default\"] * multiplier\n",
    "\n",
    "    values = []\n",
    "    approvals = []\n",
    "    for threshold in thresholds:\n",
    "        approve = val_proba < threshold\n",
    "        approve_nondefault = ((approve) & (y_val == 0)).sum()\n",
    "        approve_default = ((approve) & (y_val == 1)).sum()\n",
    "        reject_nondefault = ((~approve) & (y_val == 0)).sum()\n",
    "        reject_default = ((~approve) & (y_val == 1)).sum()\n",
    "\n",
    "        ev = (\n",
    "            approve_nondefault * payoff[\"approve_nondefault\"]\n",
    "            + approve_default * payoff[\"approve_default\"]\n",
    "            + reject_default * payoff[\"reject_default\"]\n",
    "            + reject_nondefault * payoff[\"reject_nondefault\"]\n",
    "        )\n",
    "\n",
    "        values.append(ev)\n",
    "        approvals.append(approve.mean())\n",
    "\n",
    "    best_idx = int(np.argmax(values))\n",
    "    sensitivity_rows.append(\n",
    "        {\n",
    "            \"multiplier\": multiplier,\n",
    "            \"best_threshold\": thresholds[best_idx],\n",
    "            \"best_expected_value\": values[best_idx],\n",
    "            \"best_approval_rate\": approvals[best_idx],\n",
    "        }\n",
    "    )\n",
    "\n",
    "sensitivity_df = pd.DataFrame(sensitivity_rows)\n",
    "\n",
    "sensitivity_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity results show how the optimal threshold shifts when approval payoffs are scaled by ±20%. A stable threshold indicates robust decisioning; larger shifts indicate stronger dependence on business cost assumptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:15:41.369645Z",
     "iopub.status.busy": "2026-01-22T12:15:41.369645Z",
     "iopub.status.idle": "2026-01-22T12:15:41.381644Z",
     "shell.execute_reply": "2026-01-22T12:15:41.381644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Visualize the Expected Value curve to identify the point of maximum profitability.\n",
    "# Inputs: threshold_df results.\n",
    "# Outputs: A visualization of Expected Value vs. Classification Threshold.\n",
    "# Why it matters: This plot clearly shows the \"sweet spot\" where lending decisions are most profitable under current assumptions.\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(threshold_df[\"threshold\"], threshold_df[\"expected_value\"], marker=\"o\", markersize=3)\n",
    "plt.axvline(best_threshold, color=\"red\", linestyle=\"--\", label=f\"Best threshold = {best_threshold:.2f}\")\n",
    "plt.title(\"Expected Value vs Threshold (Validation)\")\n",
    "plt.xlabel(\"Classification Threshold\")\n",
    "plt.ylabel(\"Expected Value\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "The optimal threshold from the validation expected-value curve (reported above) reflects a more conservative lending policy, because default costs are much larger than missed repayment opportunities. If default costs were lower or opportunity costs higher, the optimal threshold would shift downward to approve more loans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Takeaways\n",
    "\n",
    "Below we summarize model performance, the final model choice, and the threshold policy used for business decisioning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:15:41.381644Z",
     "iopub.status.busy": "2026-01-22T12:15:41.381644Z",
     "iopub.status.idle": "2026-01-22T12:15:41.390205Z",
     "shell.execute_reply": "2026-01-22T12:15:41.390205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_model</th>\n",
       "      <th>best_threshold</th>\n",
       "      <th>best_expected_value</th>\n",
       "      <th>best_approval_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-14</td>\n",
       "      <td>0.819582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     final_model  best_threshold  best_expected_value  best_approval_rate\n",
       "0  Random Forest            0.29                  -14            0.819582"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df\n",
    "\n",
    "threshold_summary = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"final_model\": FINAL_MODEL_NAME,\n",
    "            \"best_threshold\": best_threshold,\n",
    "            \"best_expected_value\": best_expected_value,\n",
    "            \"best_approval_rate\": best_approval_rate,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "threshold_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The final model is selected by highest validation AUC to maximize generalization performance.\n",
    "- The AUC_Gap column provides an overfitting check; smaller gaps indicate more stable models.\n",
    "- RMSE complements AUC by reflecting probability calibration quality.\n",
    "- The optimal threshold balances approval volume and default risk under the payoff assumptions.\n",
    "- Sensitivity analysis indicates how robust the threshold is to cost shifts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:15:41.390205Z",
     "iopub.status.busy": "2026-01-22T12:15:41.390205Z",
     "iopub.status.idle": "2026-01-22T12:15:41.396039Z",
     "shell.execute_reply": "2026-01-22T12:15:41.396039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python_version: 3.12.6\n",
      "pandas_version: 2.2.2\n",
      "numpy_version: 2.1.1\n",
      "sklearn_version: 1.5.2\n",
      "random_state: 42\n",
      "train_shape: (7596, 27) val_shape: (2533, 27)\n",
      "target_mean_train: 0.19286466561348078 target_mean_val: 0.19265692854322938\n"
     ]
    }
   ],
   "source": [
    "# Goal: Print the versions of key libraries used in this project.\n",
    "# Inputs: sys, sklearn, pandas, numpy modules.\n",
    "# Outputs: Version printouts for reproducibility.\n",
    "# Why it matters: Tracking versions ensures that others can recreate the exact same environment and results.\n",
    "\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "print(\"python_version:\", sys.version.split()[0])\n",
    "print(\"pandas_version:\", pd.__version__)\n",
    "print(\"numpy_version:\", np.__version__)\n",
    "print(\"sklearn_version:\", sklearn.__version__)\n",
    "print(\"random_state:\", RANDOM_STATE)\n",
    "print(\"train_shape:\", X_train.shape, \"val_shape:\", X_val.shape)\n",
    "print(\"target_mean_train:\", y_train.mean(), \"target_mean_val:\", y_val.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test Evaluation\n",
    "\n",
    "Final test evaluation is intentionally deferred to preserve an unbiased hold‑out set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook follows the full supervised learning workflow for credit risk, with clear separation between exploration, modeling, and business decisioning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Artifacts (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:15:41.396039Z",
     "iopub.status.busy": "2026-01-22T12:15:41.396039Z",
     "iopub.status.idle": "2026-01-22T12:15:42.201466Z",
     "shell.execute_reply": "2026-01-22T12:15:42.201466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_DIR: C:\\Users\\mail\\OneDrive\\Desktop\\credit-risk-ml-mif\\outputs\n",
      "approval_rate_curve_validation.png: 90.6 KB\n",
      "confusion_matrix_validation_050.csv: 0.1 KB\n",
      "confusion_matrix_validation_optimal.csv: 0.1 KB\n",
      "ev_curve_validation.png: 103.4 KB\n",
      "feature_importance_final_model.csv: 3.3 KB\n",
      "feature_importance_final_model.png: 108.3 KB\n",
      "final_report.md: 2.8 KB\n",
      "model_card.md: 2.4 KB\n",
      "model_comparison_validation.csv: 0.3 KB\n",
      "outputs_manifest.txt: 2.1 KB\n",
      "threshold_sensitivity.csv: 0.2 KB\n",
      "threshold_sweep_validation.csv: 4.0 KB\n",
      "tuning_decision_tree.csv: 0.7 KB\n",
      "tuning_gradient_boosting.csv: 0.4 KB\n",
      "tuning_random_forest.csv: 1.5 KB\n",
      "EXPORT COMPLETE\n"
     ]
    }
   ],
   "source": [
    "# Goal: Export all key results, charts, and metadata for final delivery.\n",
    "# Inputs: comparison_df, threshold_df, plots, and final pipeline.\n",
    "# Outputs: Multiple CSV, PNG, and MD files in the \"outputs\" directory, plus a manifest.\n",
    "# Why it matters: This final step packages the project artifacts, making them ready for use in reporting, auditing, or deployment.\n",
    "\n",
    "import subprocess\n",
    "import hashlib\n",
    "\n",
    "FINAL_OUTPUT_FILES = [\n",
    "    \"model_card.md\",\n",
    "    \"final_report.md\",\n",
    "    \"model_comparison_validation.csv\",\n",
    "    \"tuning_decision_tree.csv\",\n",
    "    \"tuning_random_forest.csv\",\n",
    "    \"tuning_gradient_boosting.csv\",\n",
    "    \"feature_importance_final_model.csv\",\n",
    "    \"feature_importance_final_model.png\",\n",
    "    \"ev_curve_validation.png\",\n",
    "    \"approval_rate_curve_validation.png\",\n",
    "    \"confusion_matrix_validation_050.csv\",\n",
    "    \"confusion_matrix_validation_optimal.csv\",\n",
    "    \"threshold_sweep_validation.csv\",\n",
    "    \"threshold_sensitivity.csv\",\n",
    "]\n",
    "MANIFEST_NAME = \"outputs_manifest.txt\"\n",
    "\n",
    "OPTIONAL_APPENDIX_FILES = []\n",
    "\n",
    "# Cleanup: remove existing known outputs only (clean export)\n",
    "for name in FINAL_OUTPUT_FILES + [MANIFEST_NAME]:\n",
    "    p = OUTPUT_DIR / name\n",
    "    if p.exists():\n",
    "        p.unlink()\n",
    "\n",
    "# Export tables\n",
    "comparison_df.to_csv(OUTPUT_DIR / \"model_comparison_validation.csv\", index=False)\n",
    "\n",
    "dt_tuning_df.to_csv(OUTPUT_DIR / \"tuning_decision_tree.csv\", index=False)\n",
    "rf_tuning_df.to_csv(OUTPUT_DIR / \"tuning_random_forest.csv\", index=False)\n",
    "gb_tuning_df.to_csv(OUTPUT_DIR / \"tuning_gradient_boosting.csv\", index=False)\n",
    "\n",
    "threshold_df.to_csv(OUTPUT_DIR / \"threshold_sweep_validation.csv\", index=False)\n",
    "sensitivity_df.to_csv(OUTPUT_DIR / \"threshold_sensitivity.csv\", index=False)\n",
    "\n",
    "baseline_table.to_csv(OUTPUT_DIR / \"confusion_matrix_validation_050.csv\")\n",
    "optimal_table.to_csv(OUTPUT_DIR / \"confusion_matrix_validation_optimal.csv\")\n",
    "\n",
    "# Export curves\n",
    "if \"ev_fig\" in globals():\n",
    "    ev_fig.savefig(OUTPUT_DIR / \"ev_curve_validation.png\", dpi=300, bbox_inches=\"tight\")\n",
    "if \"approval_fig\" in globals():\n",
    "    approval_fig.savefig(OUTPUT_DIR / \"approval_rate_curve_validation.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Feature importance for final model (no retraining)\n",
    "feature_names = FINAL_PIPELINE.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "model = FINAL_PIPELINE.named_steps[\"model\"]\n",
    "if hasattr(model, \"feature_importances_\"):\n",
    "    importances = model.feature_importances_\n",
    "elif hasattr(model, \"coef_\"):\n",
    "    importances = np.abs(model.coef_).ravel()\n",
    "else:\n",
    "    importances = None\n",
    "\n",
    "if importances is None:\n",
    "    raise RuntimeError(\"Final model does not expose feature importances.\")\n",
    "\n",
    "fi_df = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "fi_df = fi_df.sort_values(\"importance\", ascending=False)\n",
    "fi_df.to_csv(OUTPUT_DIR / \"feature_importance_final_model.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(\n",
    "    x=\"importance\",\n",
    "    y=\"feature\",\n",
    "    data=fi_df.head(10),\n",
    "    orient=\"h\",\n",
    ")\n",
    "plt.title(f\"Top 10 Feature Importances ({FINAL_MODEL_NAME})\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.savefig(OUTPUT_DIR / \"feature_importance_final_model.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Model card\n",
    "final_row = comparison_df.loc[comparison_df[\"Model\"] == FINAL_MODEL_NAME].iloc[0]\n",
    "model_card = f\"\"\"# Model Card\n",
    "\n",
    "## Executive Summary\n",
    "- Predicts probability of default to support lending decisions.\n",
    "- Final model selected by highest validation AUC for generalization.\n",
    "- Threshold policy balances approval volume and default risk.\n",
    "- Decisioning is grounded in transparent payoff assumptions.\n",
    "\n",
    "## Data & Split\n",
    "- Train rows: {X_train.shape[0]}\n",
    "- Validation rows: {X_val.shape[0]}\n",
    "- Test rows: {df_test.shape[0]}\n",
    "- Target prevalence (train): {y_train.mean():.4f}\n",
    "- Target prevalence (validation): {y_val.mean():.4f}\n",
    "- Random state: {RANDOM_STATE}\n",
    "\n",
    "## Feature Engineering\n",
    "- Affordability: installment_to_income, loan_to_income\n",
    "- Leverage: revol_balance_util, revol_balance_to_income\n",
    "- Stability/experience: open_to_total_acc, recent_inquiry_flag\n",
    "- Nonlinear: log_annual_inc, sqrt_dti\n",
    "\n",
    "## Models Compared\n",
    "Model performance is summarized in `model_comparison_validation.csv`. The final model is selected based on validation AUC, with AUC gap used to monitor overfitting risk.\n",
    "\n",
    "## Final Policy (Validation)\n",
    "- Final model: {FINAL_MODEL_NAME}\n",
    "- Validation AUC: {final_row['Val_AUC']:.4f}\n",
    "- Validation RMSE: {final_row['Val_RMSE']:.4f}\n",
    "- Optimal threshold: {best_threshold:.2f}\n",
    "- Approval rate: {best_approval_rate:.3f}\n",
    "- Expected value: {best_expected_value}\n",
    "\n",
    "## Confusion Matrices (Validation)\n",
    "- Threshold 0.50: approvals={baseline_approvals}, defaults among approved={baseline_bad}, approval_rate={baseline_rate:.3f}\n",
    "- Optimal threshold: approvals={optimal_approvals}, defaults among approved={optimal_bad}, approval_rate={optimal_rate:.3f}\n",
    "- Full tables are saved in `confusion_matrix_validation_050.csv` and `confusion_matrix_validation_optimal.csv`.\n",
    "\n",
    "## Assumptions & Interpretation\n",
    "The decision policy is based on stylized payoff assumptions (e.g., -5 for a default and +1 for a successful repayment). Under these assumptions, the optimal threshold is chosen to minimize expected loss (or maximize expected value). It is important to note that the resulting Expected Value (EV) may be negative if the baseline default rate is high or if the model’s discriminative power is limited. The sensitivity analysis (`threshold_sensitivity.csv`) further explores how the optimal threshold and corresponding EV shift under different cost-benefit calibrations, highlighting the dependence of the policy on these business assumptions.\n",
    "\n",
    "## Limitations & Next Steps\n",
    "- Payoff assumptions are stylized; calibrate to bank economics.\n",
    "- Threshold chosen on validation; test remains untouched for final audit.\n",
    "- Monitor drift and recalibrate thresholds as conditions change.\n",
    "- Consider calibration and fairness analyses as follow-on work.\n",
    "\n",
    "## Reproducibility\n",
    "- Run `notebooks/credit_risk_project.ipynb` → Restart Kernel & Run All.\n",
    "\"\"\"\n",
    "(OUTPUT_DIR / \"model_card.md\").write_text(model_card, encoding=\"utf-8\")\n",
    "\n",
    "# Final Report\n",
    "final_report = f\"\"\"# Credit Risk Project Final Report\n",
    "\n",
    "## Objective\n",
    "This project builds a supervised machine learning model to estimate the probability of borrower default using historical loan data, enabling data-driven lending decisions.\n",
    "\n",
    "## Data and Split Summary\n",
    "- **Training Set**: {X_train.shape[0]} samples.\n",
    "- **Validation Set**: {X_val.shape[0]} samples.\n",
    "- **Stratification**: The split maintains the original target distribution (approx. {y_train.mean():.1%} default rate) in both sets.\n",
    "\n",
    "## Leakage Controls\n",
    "- **Target Separation**: The target variable (`{TARGET_COL}`) was removed from the features (`X`) before any model training.\n",
    "- **Identifier Removal**: Unique identifiers (like `id`) were dropped to prevent models from \"memorizing\" specific rows.\n",
    "- **Hold-out Test Set**: `lending_club_test.csv` is an unlabeled hold-out file, never used for training, validation, or threshold tuning. All model selection is performed using a stratified train/validation split from the training data only.\n",
    "\n",
    "## Final Model and Performance\n",
    "- **Final Model**: {FINAL_MODEL_NAME}\n",
    "- **Validation AUC**: {final_row['Val_AUC']:.4f} (Measures ability to distinguish defaults from non-defaults)\n",
    "- **AUC Gap**: {final_row['AUC_Gap']:.4f} (Difference between train and validation performance; monitors overfitting)\n",
    "- **Validation RMSE**: {final_row['Val_RMSE']:.4f} (Measures probability calibration accuracy)\n",
    "\n",
    "Detailed model comparisons can be found in [model_comparison_validation.csv](model_comparison_validation.csv).\n",
    "\n",
    "## Threshold Decision Summary\n",
    "To translate probabilities into approval/rejection decisions, we optimized the threshold to maximize expected value under conservative payoff assumptions (Defaults are 5x more costly than missed opportunities).\n",
    "\n",
    "- **Optimal Threshold**: {best_threshold:.2f}\n",
    "- **Expected Value**: {best_expected_value} (Total validation expected value under stylized payoff units: +1 approved non-default, −1 rejected non-default, −5 approved default, 0 rejected default)\n",
    "- **Approval Rate**: {best_approval_rate:.1%} (Percentage of loans approved at this threshold)\n",
    "\n",
    "## Key Visualizations\n",
    "\n",
    "### Feature Importance\n",
    "Shows the top 10 variables driving the model's predictions.\n",
    "![Feature Importance](feature_importance_final_model.png)\n",
    "\n",
    "### Expected Value Curve\n",
    "Shows how business value changes across different probability thresholds.\n",
    "![Expected Value Curve](ev_curve_validation.png)\n",
    "\n",
    "### Approval Rate Curve\n",
    "Shows the trade-off between the classification threshold and the percentage of loans approved.\n",
    "![Approval Rate Curve](approval_rate_curve_validation.png)\n",
    "\n",
    "## Exported Artifacts\n",
    "- [Model Comparison Metrics](model_comparison_validation.csv)\n",
    "- [Threshold Sweep Results](threshold_sweep_validation.csv)\n",
    "- [Threshold Sensitivity Analysis](threshold_sensitivity.csv)\n",
    "- [Confusion Matrix (Threshold=0.50)](confusion_matrix_validation_050.csv)\n",
    "- [Confusion Matrix (Optimal Threshold)](confusion_matrix_validation_optimal.csv)\n",
    "- [Model Card](model_card.md)\n",
    "- [Outputs Manifest](outputs_manifest.txt)\n",
    "\"\"\"\n",
    "(OUTPUT_DIR / \"final_report.md\").write_text(final_report, encoding=\"utf-8\")\n",
    "\n",
    "# Outputs manifest (Deterministic: no timestamps, sorted files, git hash or unknown)\n",
    "try:\n",
    "    commit_hash = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], text=True).strip()\n",
    "except Exception:\n",
    "    commit_hash = \"unknown\"\n",
    "\n",
    "purposes = {\n",
    "    \"model_card.md\": \"Executive summary of model, metrics, and policy.\",\n",
    "    \"final_report.md\": \"Comprehensive project summary report.\",\n",
    "    \"model_comparison_validation.csv\": \"Validation metrics for all models.\",\n",
    "    \"tuning_decision_tree.csv\": \"Decision Tree tuning grid results.\",\n",
    "    \"tuning_random_forest.csv\": \"Random Forest tuning grid results.\",\n",
    "    \"tuning_gradient_boosting.csv\": \"Gradient Boosting tuning grid results.\",\n",
    "    \"feature_importance_final_model.csv\": \"Feature importances for final model.\",\n",
    "    \"feature_importance_final_model.png\": \"Top-10 feature importance chart.\",\n",
    "    \"ev_curve_validation.png\": \"Expected value vs threshold curve.\",\n",
    "    \"approval_rate_curve_validation.png\": \"Approval rate vs threshold curve.\",\n",
    "    \"confusion_matrix_validation_050.csv\": \"Confusion table at threshold=0.50.\",\n",
    "    \"confusion_matrix_validation_optimal.csv\": \"Confusion table at optimal threshold.\",\n",
    "    \"threshold_sweep_validation.csv\": \"Threshold sweep metrics on validation.\",\n",
    "    \"threshold_sensitivity.csv\": \"Sensitivity of optimal threshold to payoff shifts.\",\n",
    "}\n",
    "\n",
    "manifest_lines = [\n",
    "    f\"git_commit: {commit_hash}\",\n",
    "    \"files:\",\n",
    "]\n",
    "\n",
    "for name in sorted(FINAL_OUTPUT_FILES):\n",
    "    path = OUTPUT_DIR / name\n",
    "    if not path.exists():\n",
    "        continue\n",
    "    size_kb = path.stat().st_size / 1024\n",
    "    with open(path, \"rb\") as f:\n",
    "        sha256 = hashlib.sha256(f.read()).hexdigest()\n",
    "    manifest_lines.append(f\"- {name} | {size_kb:.1f} KB | {sha256} | {purposes.get(name, '')}\")\n",
    "\n",
    "(OUTPUT_DIR / MANIFEST_NAME).write_text(\"\\n\".join(manifest_lines), encoding=\"utf-8\")\n",
    "# Verification printout\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR.resolve()}\")\n",
    "exported_files = sorted([p for p in OUTPUT_DIR.iterdir() if p.is_file()])\n",
    "for path in exported_files:\n",
    "    size_kb = path.stat().st_size / 1024\n",
    "    print(f\"{path.name}: {size_kb:.1f} KB\")\n",
    "print(\"EXPORT COMPLETE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T12:15:42.201466Z",
     "iopub.status.busy": "2026-01-22T12:15:42.201466Z",
     "iopub.status.idle": "2026-01-22T12:15:42.219614Z",
     "shell.execute_reply": "2026-01-22T12:15:42.219614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Size</th>\n",
       "      <th>SHA256 Hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>approval_rate_curve_validation.png</td>\n",
       "      <td>90.6 KB</td>\n",
       "      <td>f3db6602d94a2146a0b62ce87cb2882d1c7f034c961fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>confusion_matrix_validation_050.csv</td>\n",
       "      <td>0.1 KB</td>\n",
       "      <td>9e6addae7e9bfe03a2deae2676ee58f2f71d72d57e0cd0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confusion_matrix_validation_optimal.csv</td>\n",
       "      <td>0.1 KB</td>\n",
       "      <td>a8ecd98fc81749a2bf75ed5b9a1b89ed712fcb3a71203e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ev_curve_validation.png</td>\n",
       "      <td>103.4 KB</td>\n",
       "      <td>53357c6b73c2ad65c80c8799477a8d3a42027ed7ed7944...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feature_importance_final_model.csv</td>\n",
       "      <td>3.3 KB</td>\n",
       "      <td>406430ba3791e5407190ddf368e92401e562dd3e2ca3b3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>feature_importance_final_model.png</td>\n",
       "      <td>108.3 KB</td>\n",
       "      <td>fa2b2e2d03840cc86bfd9e07f19322b8c18e74fb36cc90...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>final_report.md</td>\n",
       "      <td>2.8 KB</td>\n",
       "      <td>bb606a2051b96dc39817411615668ceba8e7d0c95ef600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model_card.md</td>\n",
       "      <td>2.4 KB</td>\n",
       "      <td>bec49a30d037ae00a9758c829c44570c88acc9227fd4e7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model_comparison_validation.csv</td>\n",
       "      <td>0.3 KB</td>\n",
       "      <td>84fd3ed2e1cd0b4e23002f3d7d0ee400885fbdd97ff339...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>threshold_sensitivity.csv</td>\n",
       "      <td>0.2 KB</td>\n",
       "      <td>d6e70f7de7a2f81a841bdeb9a855aff84e71511e72a0cb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>threshold_sweep_validation.csv</td>\n",
       "      <td>4.0 KB</td>\n",
       "      <td>f4015b3a56e6d008b01e1070054ef65abe673f1c86ca26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tuning_decision_tree.csv</td>\n",
       "      <td>0.7 KB</td>\n",
       "      <td>6b8349c84f1d3ec71eee92dae413231114180e36754624...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tuning_gradient_boosting.csv</td>\n",
       "      <td>0.4 KB</td>\n",
       "      <td>522dff668d8a956440e6011db9780bd191d7b0273a1140...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tuning_random_forest.csv</td>\n",
       "      <td>1.5 KB</td>\n",
       "      <td>fb25632e147e74398ae3e56afebdc08b6e63a9f22b13c3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Filename      Size  \\\n",
       "0        approval_rate_curve_validation.png   90.6 KB   \n",
       "1       confusion_matrix_validation_050.csv    0.1 KB   \n",
       "2   confusion_matrix_validation_optimal.csv    0.1 KB   \n",
       "3                   ev_curve_validation.png  103.4 KB   \n",
       "4        feature_importance_final_model.csv    3.3 KB   \n",
       "5        feature_importance_final_model.png  108.3 KB   \n",
       "6                           final_report.md    2.8 KB   \n",
       "7                             model_card.md    2.4 KB   \n",
       "8           model_comparison_validation.csv    0.3 KB   \n",
       "9                 threshold_sensitivity.csv    0.2 KB   \n",
       "10           threshold_sweep_validation.csv    4.0 KB   \n",
       "11                 tuning_decision_tree.csv    0.7 KB   \n",
       "12             tuning_gradient_boosting.csv    0.4 KB   \n",
       "13                 tuning_random_forest.csv    1.5 KB   \n",
       "\n",
       "                                          SHA256 Hash  \n",
       "0   f3db6602d94a2146a0b62ce87cb2882d1c7f034c961fee...  \n",
       "1   9e6addae7e9bfe03a2deae2676ee58f2f71d72d57e0cd0...  \n",
       "2   a8ecd98fc81749a2bf75ed5b9a1b89ed712fcb3a71203e...  \n",
       "3   53357c6b73c2ad65c80c8799477a8d3a42027ed7ed7944...  \n",
       "4   406430ba3791e5407190ddf368e92401e562dd3e2ca3b3...  \n",
       "5   fa2b2e2d03840cc86bfd9e07f19322b8c18e74fb36cc90...  \n",
       "6   bb606a2051b96dc39817411615668ceba8e7d0c95ef600...  \n",
       "7   bec49a30d037ae00a9758c829c44570c88acc9227fd4e7...  \n",
       "8   84fd3ed2e1cd0b4e23002f3d7d0ee400885fbdd97ff339...  \n",
       "9   d6e70f7de7a2f81a841bdeb9a855aff84e71511e72a0cb...  \n",
       "10  f4015b3a56e6d008b01e1070054ef65abe673f1c86ca26...  \n",
       "11  6b8349c84f1d3ec71eee92dae413231114180e36754624...  \n",
       "12  522dff668d8a956440e6011db9780bd191d7b0273a1140...  \n",
       "13  fb25632e147e74398ae3e56afebdc08b6e63a9f22b13c3...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Goal: Display a neat table of filenames, sizes, and SHA256 hashes for reproducibility.\n",
    "# Inputs: outputs/outputs_manifest.txt.\n",
    "# Outputs: A formatted DataFrame showing file details.\n",
    "# Why it matters: This provides immediate, visible proof that the files on disk match the manifest, ensuring the results are genuine and unchanged.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "manifest_path = OUTPUT_DIR / \"outputs_manifest.txt\"\n",
    "if manifest_path.exists():\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    file_data = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"- \"):\n",
    "            parts = line.strip(\"- \").strip().split(\" | \")\n",
    "            if len(parts) >= 3:\n",
    "                file_data.append({\n",
    "                    \"Filename\": parts[0].strip(),\n",
    "                    \"Size\": parts[1].strip(),\n",
    "                    \"SHA256 Hash\": parts[2].strip()\n",
    "                })\n",
    "    \n",
    "    hash_df = pd.DataFrame(file_data)\n",
    "    display(hash_df)\n",
    "else:\n",
    "    print(\"Manifest not found. Run the Export cell first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
